<h1><center>Image to Image translation using GAN</center></h1>

<h2><center>Project Proposal | DS5500 | Fall 2019</center></h2>

<h4><center>Anish Narkar, Nikhar Gaurav, Prasanna Challa, Prathwish Shetty</center></h4>


### Summary:

Image-to-Image translation is to learn a mapping between images from a source domain and images from a target domain. In this project, we investigate generative adversarial networks (GAN) for the purpose of converting satellite images to maps. The performance of traditional GAN system in detecting edges could be improved by introducing a superior edge detection architecture like U-Net to detect edges in the input, before feeding it to GAN. This could potentially improve the process of translation as the edges are well defined in the modified input, generated by feeding the actual input to U-Net.
The dataset has been obtained from pix2pix which is comprised of 1096 satellite images of New York and their corresponding google maps pages. The image translation problem involves converting satellite photos to Google maps format. We intend to leverage the Google Maps Static API (part of Google Cloud Platform) to obtain additional aerial satellite view along with its processed view to train our models.

### Proposed Plan:

The project aims to develop on top of an already existing research of Image-to-Image translation with conditional adversarial nets [1]. The satellite images will be segmented to detect the exact edges/ blocks of each entity in the image. For the purpose of segmentation, we will be using U-Net [2] which is a widely used technique in the field of medical science in applications like finding anomalies in medical images. The obtained segmented image will be superimposed on top of the satellite image to obtain a modified input with well-defined objects and sharp edges. The output of U-net will be passed through GAN to obtain the image map.
